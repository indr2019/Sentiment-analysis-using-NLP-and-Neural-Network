{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5acc9c59",
   "metadata": {},
   "source": [
    "# <font color='red'>-----------------------------------------------------------------------------------------------------</font>\n",
    "# <font color='green'>Sentiment Labelled Sentences Data Set  - Text classification</font>\n",
    "## <font color='green'>Text Classification With Python and Keras (different model implementations)</font>\n",
    "# <font color='red'>-----------------------------------------------------------------------------------------------------</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ff04c5",
   "metadata": {},
   "source": [
    "# <ins><div class=\"alert alert-block alert-info\">*Objective: -*</div></ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9395f76",
   "metadata": {},
   "source": [
    "### Customer Sentiment analysis from text reviews with the help of NLP (NLTK and Word2vec and modelling using Convolution neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30372336",
   "metadata": {},
   "source": [
    "## <ins><div class=\"alert alert-block alert-info\">*Part 1: Extracting the dataset*</div></ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091fdd0d",
   "metadata": {},
   "source": [
    "### <ins><div class=\"alert alert-block alert-warning\">*Step 1: Reading the text file to create a dataframe*</div></ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90055675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09fef5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the files from the data source and creating dataframe with sentence, label, source\n",
    "text_dict = {'yelp':r'C:\\Users\\Indranil\\sentiment labelled sentences\\sentiment labelled sentences\\yelp_labelled.txt',\n",
    "            'imdb':r'C:\\Users\\Indranil\\sentiment labelled sentences\\sentiment labelled sentences\\imdb_labelled.txt',\n",
    "            'amazon':r'C:\\Users\\Indranil\\sentiment labelled sentences\\sentiment labelled sentences\\amazon_cells_labelled.txt'}\n",
    "\n",
    "reviews_list = []\n",
    "for source, filepath in text_dict.items():\n",
    "    reviews = pd.read_csv(filepath, names=['sentence', 'liked'], sep='\\t')\n",
    "    reviews['source'] = source  # Adding column for the source name\n",
    "    reviews_list.append(reviews)\n",
    "\n",
    "reviews = pd.concat(reviews_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a915ba6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2748, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce027be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>liked</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  liked source\n",
       "0                           Wow... Loved this place.      1   yelp\n",
       "1                                 Crust is not good.      0   yelp\n",
       "2          Not tasty and the texture was just nasty.      0   yelp\n",
       "3  Stopped by during the late May bank holiday of...      1   yelp\n",
       "4  The selection on the menu was great and so wer...      1   yelp"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055ce31e",
   "metadata": {},
   "source": [
    "Our dataframe is built with the statements written as reviews, the ratings 'liked' and the source or the platform where the review was provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8edd6fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "sns.countplot(reviews['liked'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff7cb62",
   "metadata": {},
   "source": [
    "#### We seem to have a pretty balanced data set in terms of -ve and +ve sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beb9d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting index since dataframe is made with 3 sources and index is getting reset after end of items of the previous source\n",
    "reviews.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c98f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping specific rows since during the pre processing those rows got converted to empty string\n",
    "reviews.drop(reviews.index[[140,1064,1590]],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81f4bb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2745, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd77f4e",
   "metadata": {},
   "source": [
    "### <ins><div class=\"alert alert-block alert-warning\">*Step 2:Text pre-processing*</div></ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee917a2",
   "metadata": {},
   "source": [
    "Text preprocessing steps to remove unwanted elemenmts like emails, special char etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "075140d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Indranil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Indranil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing libraries for text pre processing\n",
    "import re \n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wrd_lemma = WordNetLemmatizer()\n",
    "\n",
    "all_stpwrds = stopwords.words('english')\n",
    "all_stpwrds.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e84fb215",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the data\n",
    "corpus = []\n",
    "for i in reviews['sentence']:\n",
    "      \n",
    "    cleaned = re.sub('[^a-zA-Z]',' ',i)\n",
    "    cleaned = cleaned.lower()\n",
    "    cleaned = cleaned.split()\n",
    "    cleaned = [wrd_lemma.lemmatize(word) for word in cleaned if not word in set(all_stpwrds)]\n",
    "    cleaned = ' '.join(cleaned)\n",
    "    corpus.append(cleaned) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e2c27a",
   "metadata": {},
   "source": [
    "Looks like we have a cleaned dataset with preprocessed texts of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f2326eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e420aad6",
   "metadata": {},
   "source": [
    "### <ins><div class=\"alert alert-block alert-warning\">*Step 3: Processing the corpus for model building and word embedding*</div></ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45e72ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining X and y dataset\n",
    "X = corpus\n",
    "y = reviews['liked']\n",
    "\n",
    "# splitting the data into test and training data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.30,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02b4eed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries for deep learning models\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4934be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating a tokenizer\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d684af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the word index\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a38a228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting strings into list of integer indices\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62330c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying word vocabulary\n",
    "word_vocab = len(tokenizer.word_index)+1\n",
    "# print(tokenizer.word_index)\n",
    "# print(\"Found %s unique words\" % word_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58de9a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n"
     ]
    }
   ],
   "source": [
    "# # padding the sequences for having equal length of input to the model\n",
    "\n",
    "# finding the largest review sentence\n",
    "max_len=[]\n",
    "for i in X_train:\n",
    "    i = i.split()\n",
    "    max_len.append(len(i))\n",
    "\n",
    "max_length = max(max_len)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8aacc6",
   "metadata": {},
   "source": [
    "The largest sequence in the list is 684. We shall use that as max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45be0797",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(X_train_seq,maxlen=max_length,padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq,maxlen=max_length,padding='post')\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)\n",
    "# print(\"Shape of the pad_data Tensor\",X_test_pad.shape)\n",
    "# print(\"Shape of the label Tensor\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe51a96",
   "metadata": {},
   "source": [
    "### <ins><div class=\"alert alert-block alert-warning\">*Step 4:Data transformation: Transform the cleaned dataset into vector of words using word embeddings*</div></ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f9e17b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading word2vec word embeddings\n",
    "import gensim\n",
    "wordembeddings = gensim.models.KeyedVectors.load_word2vec_format('D:/GoogleNews-vectors-negative300.bin',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "407532b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_words = word_vocab\n",
    "embed_dim = 300\n",
    "embed_matrix = np.zeros((tot_words,embed_dim))\n",
    "skipped_words = []\n",
    "embed_vect = np.empty(shape=(300,))\n",
    "\n",
    "for words,index in tokenizer.word_index.items():\n",
    "    ######## word embedings dont have OOV word and hence will throw error\n",
    "    if words in wordembeddings:\n",
    "        embed_vect = wordembeddings[words]\n",
    "    else:\n",
    "        skipped_words.append(words)\n",
    "        \n",
    "    embed_matrix[index] = embed_vect        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fe0291",
   "metadata": {},
   "source": [
    "### <ins><div class=\"alert alert-block alert-warning\">*Step 5:Model building: Fitting Deep learning model, model testing*</div></ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f37d1605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the embedding layer\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "embed_layr = Embedding(tot_words,embed_dim,weights=[embed_matrix],input_length=max_length,trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1719da8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 404, 300)          1068900   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 403, 64)           38464     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 201, 64)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 200, 128)          16512     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 100, 128)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 99, 256)           65792     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 49, 256)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 48, 512)           262656    \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 24, 512)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12288)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                393248    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,845,913\n",
      "Trainable params: 777,013\n",
      "Non-trainable params: 1,068,900\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# building the model\n",
    "model = Sequential()\n",
    "\n",
    "# building the convolution layers\n",
    "model.add(embed_layr)\n",
    "model.add(Conv1D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=256, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=512, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "\n",
    "# building the dense layers\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ace306",
   "metadata": {},
   "source": [
    "**Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5eaedef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      "31/31 - 7s - loss: 0.6924 - accuracy: 0.4971 - 7s/epoch - 216ms/step\n",
      "Epoch 2/9\n",
      "31/31 - 5s - loss: 0.5953 - accuracy: 0.6726 - 5s/epoch - 171ms/step\n",
      "Epoch 3/9\n",
      "31/31 - 5s - loss: 0.4754 - accuracy: 0.8022 - 5s/epoch - 172ms/step\n",
      "Epoch 4/9\n",
      "31/31 - 6s - loss: 0.3669 - accuracy: 0.8532 - 6s/epoch - 180ms/step\n",
      "Epoch 5/9\n",
      "31/31 - 6s - loss: 0.3007 - accuracy: 0.8954 - 6s/epoch - 196ms/step\n",
      "Epoch 6/9\n",
      "31/31 - 6s - loss: 0.2032 - accuracy: 0.9386 - 6s/epoch - 182ms/step\n",
      "Epoch 7/9\n",
      "31/31 - 6s - loss: 0.1496 - accuracy: 0.9531 - 6s/epoch - 187ms/step\n",
      "Epoch 8/9\n",
      "31/31 - 6s - loss: 0.1295 - accuracy: 0.9573 - 6s/epoch - 196ms/step\n",
      "Epoch 9/9\n",
      "31/31 - 7s - loss: 0.0799 - accuracy: 0.9792 - 7s/epoch - 225ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2731201ce80>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.optimizers import *\n",
    "\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(X_train_pad, y_train, batch_size=64,epochs=9, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38a075b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 - 1s - loss: 0.6114 - accuracy: 0.8143 - 1s/epoch - 45ms/step\n",
      "Test Accuracy: 81.432039\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "loss, acc = model.evaluate(X_test_pad, y_test, verbose=2)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca30e252",
   "metadata": {},
   "source": [
    "**Training accuracy is at ~90% plus and Test accuracy is at ~82% plus. Our model has worked appropriately**<br>\n",
    "**Let's test our model by giving some reviews as example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3300d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = str(input(\"Enter review to find sentiment: \"))\n",
    "# a = [inp]\n",
    "# a_seq = tokenizer.texts_to_sequences(a)\n",
    "# a_pad = pad_sequences(a_seq,maxlen=max_length,padding='post')\n",
    "# predict = model.predict(a_pad)\n",
    "# print(predict)\n",
    "# if predict > 0.5:\n",
    "#     print(a,'\\n')\n",
    "#     print(\"Positive sentiment\")\n",
    "# else:\n",
    "#     print('Negative sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843d72e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
